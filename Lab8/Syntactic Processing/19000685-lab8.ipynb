{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c459151-a4ca-4504-a1a8-4cafab5286c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CLASSPATH'] = 'C:/Program Files/stanford-parser-full-2020-11-17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b479ed-f723-4847-b4dc-6209b4191f45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\285918331.py:13: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  scp = StanfordParser(model_path='C:/Program Files/stanford-parser-full-2020-11-17/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (DT the) (JJ quick) (JJ red) (NN riding) (NN food))\n",
      "    (VP\n",
      "      (VBD jumped)\n",
      "      (PP (IN over) (NP (DT the) (JJ small) (NN log))))))\n",
      "(S\n",
      "  (NP-SBJ (NNP Mr.) (NNP Vinken))\n",
      "  (VP\n",
      "    (VBZ is)\n",
      "    (NP-PRD\n",
      "      (NP (NN chairman))\n",
      "      (PP\n",
      "        (IN of)\n",
      "        (NP\n",
      "          (NP (NNP Elsevier) (NNP N.V.))\n",
      "          (, ,)\n",
      "          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n",
      "  (. .))\n",
      "[('the', 'DT'), ('quick', 'JJ'), ('red', 'JJ'), ('riding', 'NN'), ('food', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('small', 'JJ'), ('log', 'NN')]\n",
      "(S\n",
      "  (NP-SBJ\n",
      "    (DT the)\n",
      "    (JJ quick)\n",
      "    (JJ red)\n",
      "    (NN riding)\n",
      "    (NN food))\n",
      "  (ADVP-PRD\n",
      "    (VBN jumped)\n",
      "    (PP (IN over) (NP-LGS (DT the) (JJ small) (NN log))))) (p=7.38607e-37)\n"
     ]
    }
   ],
   "source": [
    "# constituency_parsing.py\n",
    "sentence = 'the quick red riding food jumped over the small log'\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "scp = StanfordParser(model_path='C:/Program Files/stanford-parser-full-2020-11-17/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')\n",
    "\n",
    "\n",
    "\n",
    "result = list(scp.raw_parse(sentence))\n",
    "print(result[0])\n",
    "\n",
    "result[0].draw()\n",
    "\n",
    "import nltk\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "training_set = treebank.parsed_sents()\n",
    "\n",
    "print(training_set[1])\n",
    "\n",
    "# extract the productions for all annotated training sentences\n",
    "treebank_productions = list(\n",
    "                        set(production \n",
    "                            for sent in training_set  \n",
    "                            for production in sent.productions()\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "treebank_productions[0:10]\n",
    "  \n",
    "# add productions for each word, POS tag\n",
    "for word, tag in treebank.tagged_words():\n",
    "\tt = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "\tfor production in t.productions():\n",
    "\t\ttreebank_productions.append(production)\n",
    "\n",
    "# build the PCFG based grammar  \n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)\n",
    "\n",
    "# build the parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get sample sentence tokens\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "\n",
    "# get tokens and their POS tags from pattern package\n",
    "from pattern.en import tag as pos_tagger\n",
    "tagged_sent = pos_tagger(sentence)\n",
    "\n",
    "# use NLTK POS tagger instead\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tagged_sent)\n",
    "\n",
    "# extend productions for sample sentence tokens\n",
    "for word, tag in tagged_sent:\n",
    "    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n",
    "\n",
    "# rebuild grammar\n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)                                         \n",
    "\n",
    "# rebuild parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "print(result[0])\n",
    "result[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f14d93-1224-420f-940b-a3a8f1f548ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRON'), ('saw', 'VERB'), ('the', 'DET'), ('man', 'NOUN'), ('with', 'ADP'), ('the', 'DET'), ('telescope', 'NOUN'), ('but', 'CONJ'), ('he', 'PRON'), ('did', 'VERB'), (\"n't\", 'ADV'), ('see', 'VERB'), ('me', 'PRON')]\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:39: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(dt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1454158195372253\n",
      "[('I', 'NN'), ('saw', 'NN'), ('the', 'NN'), ('man', 'NN'), ('with', 'NN'), ('the', 'NN'), ('telescope', 'NN'), ('but', 'NN'), ('he', 'NN'), ('did', 'NN'), (\"n't\", 'NN'), ('see', 'NN'), ('me', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:59: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(rt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24039113176493368\n",
      "[('I', 'NN'), ('saw', 'NN'), ('the', 'NN'), ('man', 'NN'), ('with', 'NN'), ('the', 'NN'), ('telescope', 'NN'), ('but', 'NN'), ('he', 'NN'), ('did', 'NN'), (\"n't\", 'NN'), ('see', 'NN'), ('me', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:79: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(ut.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8607803272340013\n",
      "[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', None), ('but', 'CC'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('see', 'VB'), ('me', 'PRP')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:83: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(bt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13466937748087907\n",
      "[('I', 'PRP'), ('saw', None), ('the', None), ('man', None), ('with', None), ('the', None), ('telescope', None), ('but', None), ('he', None), ('did', None), (\"n't\", None), ('see', None), ('me', None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:87: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(tt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08064672281924679\n",
      "[('I', 'PRP'), ('saw', None), ('the', None), ('man', None), ('with', None), ('the', None), ('telescope', None), ('but', None), ('he', None), ('did', None), (\"n't\", None), ('see', None), ('me', None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:101: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(ct.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9094781682641108\n",
      "[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', 'NN'), ('but', 'CC'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('see', 'VB'), ('me', 'PRP')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:118: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(nbt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306806079969019\n",
      "[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', 'NN'), ('but', 'CC'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('see', 'VB'), ('me', 'PRP')]\n",
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.82864        0.007\n",
      "             2          -0.76176        0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\classify\\maxent.py:1381: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2**nf_delta\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\classify\\maxent.py:1383: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\classify\\maxent.py:1384: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Final               nan        0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:130: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(met.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9270016458514861\n",
      "[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', 'NN'), ('but', 'CC'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('see', 'VB'), ('me', 'PRP')]\n",
      "Tagger accuracies:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:137: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Default tagger %.2f' %dt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default tagger 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:138: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Regex tagger %.2f' %rt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex tagger 0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:139: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Unigram tagger %.2f' %ut.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram tagger 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:140: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Bigram tagger %.2f' %bt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram tagger 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:141: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Trigram tagger %.2f' %tt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram tagger 0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:142: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Combined tagger %.2f' %ct.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined tagger 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:143: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Naive Bayes tagger %.2f' %nbt.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes tagger 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\3240707400.py:144: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print('Maxent tagger %.2f' %met.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxent tagger 0.93\n"
     ]
    }
   ],
   "source": [
    "# pos_tagging.py\n",
    "sentence = \"I saw the man with the telescope but he didn't see me\"\n",
    "\n",
    "\n",
    "# Using NLTK's built-in tagger based on PTB\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print(tagged_sent)\n",
    "\n",
    "    \n",
    "# Using the pattern package (Python 2.x only) built-in tagger (optional)\n",
    "# from pattern.en import tag\n",
    "# tagged_sent = tag(sentence)\n",
    "# print(tagged_sent)\n",
    "\n",
    "\n",
    "\n",
    "# Building your own tagger\n",
    "# - default tagger that tags all words the same!\n",
    "# - regex tagger that doesn't care about context (most common tag per word)\n",
    "\n",
    "# Fortunately the treebank corpus is bundled with NLTK for training a tagger\n",
    "# We need to divide the data into training and test sets first\n",
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data[0])\n",
    "\n",
    "# SAQ 1: How much data is there for training, testing?\n",
    "# SAQ 2: What is the last training sentence; test sentence?\n",
    "\n",
    "# Default 'naive' tagger - tags all words with a given tag!\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN') # Can specify any default tag - NN gives best score - why?\n",
    "\n",
    "# Test score and example sentence tag output\n",
    "print(dt.evaluate(test_data))\n",
    "print(dt.tag(tokens))\n",
    "\n",
    "\n",
    "# Regex tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "# Define 'fixed' regex tag patterns\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),               # gerunds\n",
    "        (r'.*ed$', 'VBD'),                # simple past\n",
    "        (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),               # modals\n",
    "        (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                 # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                     # nouns (default) ... \n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "\n",
    "# Test score and example sentence tag output\n",
    "print(rt.evaluate(test_data))\n",
    "print(rt.tag(tokens))\n",
    "\n",
    "\n",
    "# Training your own tagger\n",
    "# 1. using n-gram taggers and combining them with backoff\n",
    "# 2. using naive bayes (statistical) model\n",
    "# 3. using maximum entropy (classifier) model\n",
    "\n",
    "## N gram taggers\n",
    "from nltk.tag import UnigramTagger # Context insentitive\n",
    "from nltk.tag import BigramTagger  # Considers previous word\n",
    "from nltk.tag import TrigramTagger # Considers previous 2 words\n",
    "\n",
    "# Traing the taggers\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "\n",
    "# Test UnigramTagger score and example sentence tag output\n",
    "print(ut.evaluate(test_data))\n",
    "print(ut.tag(tokens))\n",
    "\n",
    "# Test BigramTagger score and example sentence tag output\n",
    "print(bt.evaluate(test_data))\n",
    "print(bt.tag(tokens))\n",
    "\n",
    "# Test TrigramTagger score and example sentence tag output\n",
    "print(tt.evaluate(test_data))\n",
    "print(tt.tag(tokens))\n",
    "\n",
    "# Combining all 3 n-gram taggers with backoff (smoothing)\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data, \n",
    "                     taggers=[UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff=rt)\n",
    "\n",
    "# Test Combined n-gram tagger score and example sentence tag output\n",
    "print(ct.evaluate(test_data))       \n",
    "print(ct.tag(tokens))\n",
    "\n",
    "\n",
    "\n",
    "# Treating POS tagging as a classification problem\n",
    "# We use the ClassifierBasedPOSTagger class to build a classifier by specifying some\n",
    "# classification algorithm - here the NaiveBayes abd Maxent algorithms which are passed\n",
    "# to the class via the classifier_builder parameter\n",
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "# First a Naive Bayes (statistical) classifier\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n",
    "\n",
    "# Test NBC tagger score and example sentence tag output\n",
    "print(nbt.evaluate(test_data))\n",
    "print(nbt.tag(tokens))    \n",
    "\n",
    "\n",
    "# Finally a Maximum entropy classifier (that would take sometime)\n",
    "# met = ClassifierBasedPOSTagger(train=train_data,\n",
    "#                               classifier_builder=MaxentClassifier.train)\n",
    "\n",
    "met = ClassifierBasedPOSTagger(train=train_data, \n",
    "                               classifier_builder=lambda train_feats: MaxentClassifier.train(train_feats, max_iter=10))\n",
    "\n",
    "# Test Maxent tagger score and example sentence tag output\n",
    "print(met.evaluate(test_data))                           \n",
    "print(met.tag(tokens))\n",
    "\n",
    "\n",
    "# Final accuracies\n",
    "print('Tagger accuracies:')\n",
    "print()\n",
    "print('Default tagger %.2f' %dt.evaluate(test_data))\n",
    "print('Regex tagger %.2f' %rt.evaluate(test_data))\n",
    "print('Unigram tagger %.2f' %ut.evaluate(test_data))\n",
    "print('Bigram tagger %.2f' %bt.evaluate(test_data))\n",
    "print('Trigram tagger %.2f' %tt.evaluate(test_data))\n",
    "print('Combined tagger %.2f' %ct.evaluate(test_data))\n",
    "print('Naive Bayes tagger %.2f' %nbt.evaluate(test_data))\n",
    "print('Maxent tagger %.2f' %met.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43043e6f-fea3-4bec-a41f-313570c33b39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n",
      "[('the', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
      "(S\n",
      "  (NP the/DT brown/JJ fox/NN)\n",
      "  (VP jumped/VBD over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "(S\n",
      "  the/DT\n",
      "  (NP brown/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  the/DT\n",
      "  (NP lazy/JJ dog/NN))\n",
      "(S\n",
      "  (NP the/DT brown/JJ fox/NN)\n",
      "  (VP is/VBZ)\n",
      "  (ADJP quick/JJ)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  (VP may/MD jump/VB)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\1847143938.py:63: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(rc.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  46.1%%\n",
      "    Precision:     19.9%%\n",
      "    Recall:        43.3%%\n",
      "    F-Measure:     27.3%%\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\1847143938.py:119: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(ntc.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.2%%\n",
      "    Precision:     91.4%%\n",
      "    Recall:        94.3%%\n",
      "    F-Measure:     92.8%%\n",
      "(S\n",
      "  (NP the/DT brown/JJ fox/NN)\n",
      "  is/VBZ\n",
      "  (NP quick/JJ)\n",
      "  and/CC\n",
      "  (NP he/PRP)\n",
      "  may/MD\n",
      "  jump/VB\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\1847143938.py:141: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(tc.evaluate(test_wsj_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.4%%\n",
      "    Precision:     80.8%%\n",
      "    Recall:        86.0%%\n",
      "    F-Measure:     83.3%%\n"
     ]
    }
   ],
   "source": [
    "#shallow_parsing.py\n",
    "\n",
    "# Training your own chunker using chunked treeband data - again made available in NLTK!\n",
    "# As before (with tagging) we first divide the data into training and testing sets\n",
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[0:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data[0])\n",
    "\n",
    "simple_sentence = 'the brown fox jumped over the lazy dog'\n",
    "\n",
    "# Can use tagger from package pattern.en if using Python 2.x\n",
    "# from pattern.en import tag\n",
    "# tagged_sentence = tag(sentence)\n",
    "\n",
    "import nltk\n",
    "from nltk.chunk import RegexpParser\n",
    "tokens = nltk.word_tokenize(simple_sentence)\n",
    "tagged_simple_sent = nltk.pos_tag(tokens)\n",
    "print(tagged_simple_sent)\n",
    "\n",
    "# We first define our grammars using regex pattern using the RegexpParser\n",
    "# We can specify which patterns we want to segment in a sentence as *chunks*\n",
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "VP: {<VBD><IN>}\n",
    "\"\"\"\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)\n",
    "\n",
    "# We sometimes want to specify which patterns we DO NOT want to segment in a sentence\n",
    "# so that we can *chunk* all the others\n",
    "chink_grammar = \"\"\"\n",
    "NP: {<JJ|NN>+} # chunk only adjective-noun pair as NP\n",
    "\"\"\"\n",
    "\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)\n",
    "\n",
    "\n",
    "# A more realistic grammar for chunking\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}  \n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}      \n",
    "VP: {<MD>?<VB.*>+}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# And a more realistic sentence as input\n",
    "sentence = 'the brown fox is quick and he may jump over the lazy dog'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens)\n",
    "\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_sent)\n",
    "print(c)\n",
    "\n",
    "print(rc.evaluate(test_data))\n",
    "# The performance is not great!\n",
    "# Why is this?\n",
    "\n",
    "\n",
    "# We have acecss to a utility function tree2conlltags which extracts word, tag and\n",
    "# chunk triples from annotated text\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "# Let's take a slightly more typical sentence from our data\n",
    "train_sent = train_data[7]\n",
    "print(train_sent)\n",
    "\n",
    "# We extract the POS and chunk tags using tree2conlltags function which returns a list of tuples\n",
    "wtc = tree2conlltags(train_sent)\n",
    "wtc\n",
    "\n",
    "# We can 'reverse' this to output a shallow tree using the conlltags2tree function\n",
    "tree = conlltags2tree(wtc)\n",
    "print(tree)\n",
    "\n",
    "\n",
    "# We can use these features to train a 'combined' chunker as we did for POS tagging\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "  tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "  \n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "  \n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "# We create a new class to use the word, POS and Chunk tag features to train a chunker\n",
    "# that is able to 'backoff' from bigram to a unigram model as before\n",
    "# Can you have another layer for trigram and back off to this model?\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    \n",
    "  def __init__(self, train_sentences, \n",
    "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "  def parse(self, tagged_sentence):\n",
    "    if not tagged_sentence: \n",
    "        return None\n",
    "    pos_tags = [tag for word, tag in tagged_sentence]\n",
    "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                     in zip(tagged_sentence, chunk_tags)]\n",
    "    return conlltags2tree(wpc_tags)\n",
    "\n",
    "# We call our new class and pass it the training data from the chunked treebank\n",
    "ntc = NGramTagChunker(train_data)\n",
    "print(ntc.evaluate(test_data))\n",
    "# Now we get really good results on the data set\n",
    "# Why?\n",
    "\n",
    "# Let's try to visualize the chunk for the more realistic sample sentence\n",
    "tree = ntc.parse(tagged_sent)\n",
    "print(tree)\n",
    "tree.draw()\n",
    "\n",
    "\n",
    "# We now use our shallow parser on a larger 'Wall Street Journal' corpus\n",
    "# SAQ 1. How big is it?\n",
    "# SAQ 2. How much test data did we have before, and how much now?\n",
    "from nltk.corpus import conll2000\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:7500]\n",
    "test_wsj_data = wsj_data[7500:]\n",
    "print(train_wsj_data[10])\n",
    "\n",
    "# We first train our model on the training corpus\n",
    "tc = NGramTagChunker(train_wsj_data)\n",
    "# And then we test it on the test data\n",
    "print(tc.evaluate(test_wsj_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bfa33ab-34b3-4c04-8098-a3526898092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]<---Colorless[]--->[]\n",
      "--------\n",
      "[]<---green[]--->[]\n",
      "--------\n",
      "[]<---ideas[]--->[]\n",
      "--------\n",
      "[]<---sleep[]--->[]\n",
      "--------\n",
      "[]<---furiously[]--->[]\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11252\\4249957290.py:38: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  sdp = StanfordDependencyParser(model_path='C:/Program Files/stanford-parser-full-2020-11-17/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x000001580F0868C0>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [4]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'Colorless'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 3,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'green'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'NNS',\n",
      "                 'deps': defaultdict(<class 'list'>, {'amod': [1, 2]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NNS',\n",
      "                 'word': 'ideas'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'VBP',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'advmod': [5],\n",
      "                                      'nsubj': [3]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'VBP',\n",
      "                 'word': 'sleep'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'RB',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'advmod',\n",
      "                 'tag': 'RB',\n",
      "                 'word': 'furiously'}})\n",
      "Dependency grammar with 12 productions\n",
      "  'fox' -> 'The'\n",
      "  'fox' -> 'brown'\n",
      "  'quick' -> 'fox'\n",
      "  'quick' -> 'is'\n",
      "  'quick' -> 'and'\n",
      "  'quick' -> 'jumping'\n",
      "  'jumping' -> 'he'\n",
      "  'jumping' -> 'is'\n",
      "  'jumping' -> 'dog'\n",
      "  'dog' -> 'over'\n",
      "  'dog' -> 'the'\n",
      "  'dog' -> 'lazy'\n",
      "No valid parse trees were found.\n"
     ]
    }
   ],
   "source": [
    "# dependency_parsing.py\n",
    "# os.environ['PATH'] = 'C:/Program Files/Graphviz/bin'\n",
    "sentence = 'Colorless green ideas sleep furiously'\n",
    "\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "parsed_sent = parser(sentence)\n",
    "\n",
    "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
    "for token in parsed_sent:\n",
    "    print(dependency_pattern.format(word=token.orth_, \n",
    "                                  w_type=token.dep_,\n",
    "                                  left=[t.orth_ \n",
    "                                            for t \n",
    "                                            in token.lefts],\n",
    "                                  right=[t.orth_ \n",
    "                                             for t \n",
    "                                             in token.rights]))\n",
    "                                             \n",
    "\n",
    "# Since the Stanford Dependency parser is written in Java, we need to:\n",
    "# 0. Install Java 1.8 or better\n",
    "# 1. Download the Stanford Parser from https://nlp.stanford.edu/software/lex-parser.shtml\n",
    "# 2. Unzip it to a directory and\n",
    "# 3. Set environment variables to our JAVA path and this direcory\n",
    "# import os\n",
    "# java_path = r'/usr/bin/java'\n",
    "# os.environ['JAVAHOME'] = java_path\n",
    "                                             \n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "\"\"\"\n",
    "Previous version of NLTK/Stanford Parser syntax\n",
    "sdp = StanfordDependencyParser(path_to_jar='/Users/arw/Documents/Work/stanford-parser-full/stanford-parser.jar',\n",
    "                               model_path='/Users/arw/Documents/Work/stanford-parser-full/stanford-parser-3.8.0-models.jar')\n",
    "\"\"\"\n",
    "\n",
    "sdp = StanfordDependencyParser(model_path='C:/Program Files/stanford-parser-full-2020-11-17/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')\n",
    "\n",
    "\n",
    "result = list(sdp.raw_parse(sentence))  \n",
    "\n",
    "\"\"\"\n",
    "result[0]\n",
    "\n",
    "[item for item in result[0]]\n",
    "\"\"\"\n",
    "\n",
    "dep_tree = [parse for parse in result][0]\n",
    "print(dep_tree)\n",
    "dep_tree\n",
    "\n",
    "# generation of annotated dependency tree\n",
    "from graphviz import Source\n",
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree\", format=\"png\")\n",
    "source.view()\n",
    "             \n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "dependency_rules = \"\"\"\n",
    "'fox' -> 'The' | 'brown'\n",
    "'quick' -> 'fox' | 'is' | 'and' | 'jumping'\n",
    "'jumping' -> 'he' | 'is' | 'dog'\n",
    "'dog' -> 'over' | 'the' | 'lazy'\n",
    "\"\"\"\n",
    "\n",
    "dependency_grammar = nltk.grammar.DependencyGrammar.fromstring(dependency_rules)\n",
    "print(dependency_grammar)\n",
    "\n",
    "dp = nltk.ProjectiveDependencyParser(dependency_grammar)\n",
    "res = [item for item in dp.parse(tokens)]\n",
    "if len(res) > 0:\n",
    "    tree = res[0] \n",
    "    print(tree)\n",
    "    tree.draw()\n",
    "else:\n",
    "    print(\"No valid parse trees were found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569bfbb5-2d84-4d42-b007-94beecbe5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the man with the telescope\n",
      "[Chunk('I/NP'), Chunk('saw/VP'), Chunk('the man/NP'), Chunk('with/PP'), Chunk('the telescope/NP')]\n",
      "NP -> [('I', 'PRP')]\n",
      "VP -> [('saw', 'VBD')]\n",
      "NP -> [('the', 'DT'), ('man', 'NN')]\n",
      "PP -> [('with', 'IN')]\n",
      "NP -> [('the', 'DT'), ('telescope', 'NN')]\n",
      "I saw the man with the telescope\n",
      "(S\n",
      "  (NP (PRP I))\n",
      "  (VP (VBD saw))\n",
      "  (NP (DT the) (NN man))\n",
      "  (PP (IN with))\n",
      "  (NP (DT the) (NN telescope)))\n"
     ]
    }
   ],
   "source": [
    "# shallow_parsing_with_pattern.py\n",
    "\n",
    "from pattern.en import parsetree, Chunk\n",
    "from nltk.tree import Tree\n",
    "\n",
    "sentence = 'I saw the man with the telescope'\n",
    "\n",
    "tree = parsetree(sentence)\n",
    "print(tree)\n",
    "\n",
    "for sentence_tree in tree:\n",
    "    print(sentence_tree.chunks)\n",
    "    \n",
    "for sentence_tree in tree:\n",
    "    for chunk in sentence_tree.chunks:\n",
    "        print(chunk.type, '->', [(word.string, word.type) \n",
    "                                 for word in chunk.words])\n",
    "        \n",
    "\n",
    "def create_sentence_tree(sentence, lemmatize=False):\n",
    "    sentence_tree = parsetree(sentence, \n",
    "                              relations=True, \n",
    "                              lemmata=lemmatize)\n",
    "    return sentence_tree[0]\n",
    "    \n",
    "def get_sentence_tree_constituents(sentence_tree):\n",
    "    return sentence_tree.constituents()\n",
    "    \n",
    "def process_sentence_tree(sentence_tree):\n",
    "    \n",
    "    tree_constituents = get_sentence_tree_constituents(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        (item.type,\n",
    "                         [\n",
    "                             (w.string, w.type)\n",
    "                             for w in item.words\n",
    "                         ]\n",
    "                        )\n",
    "                        if type(item) == Chunk\n",
    "                        else ('-',\n",
    "                              [\n",
    "                                   (item.string, item.type)\n",
    "                              ]\n",
    "                             )\n",
    "                             for item in tree_constituents\n",
    "                    ]\n",
    "    \n",
    "    return processed_tree\n",
    "    \n",
    "def print_sentence_tree(sentence_tree):\n",
    "    \n",
    "\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        Tree( item[0],\n",
    "                             [\n",
    "                                 Tree(x[1], [x[0]])\n",
    "                                 for x in item[1]\n",
    "                             ]\n",
    "                            )\n",
    "                            for item in processed_tree\n",
    "                     ]\n",
    "\n",
    "    tree = Tree('S', processed_tree )\n",
    "    print(tree)\n",
    "    \n",
    "def visualize_sentence_tree(sentence_tree):\n",
    "    \n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "                        Tree( item[0],\n",
    "                             [\n",
    "                                 Tree(x[1], [x[0]])\n",
    "                                 for x in item[1]\n",
    "                             ]\n",
    "                            )\n",
    "                            for item in processed_tree\n",
    "                     ]\n",
    "    tree = Tree('S', processed_tree )\n",
    "    tree.draw()\n",
    "    \n",
    "    \n",
    "t = create_sentence_tree(sentence)\n",
    "print(t)\n",
    "\n",
    "pt = process_sentence_tree(t)\n",
    "pt\n",
    "\n",
    "print_sentence_tree(t)\n",
    "visualize_sentence_tree(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5a8a1-dba9-4d9c-a143-21cb4fdbd0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
