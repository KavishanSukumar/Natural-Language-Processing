{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430a492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5728f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=brown.raw(brown.fileids()[9])\n",
    "sentences=sent_tokenize(text)\n",
    "newsentence=text.replace('/n', ' ')\n",
    "newsentence=newsentence.replace('/', ' ')\n",
    "words = word_tokenize(newsentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dfa94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df311ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b24eae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60ac1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84b2a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a940e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acc648d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ben PERSON\n",
      "pps ORG\n",
      "Monday DATE\n",
      "Charles p E. p Raymond p ORG\n",
      "District n-tl ORG\n",
      "Ierulli p ORG\n",
      "Desmond p D. p Connall p ORG\n",
      "ben PERSON\n",
      "March DATE\n",
      "31 CARDINAL\n",
      "Ierulli p ORG\n",
      "29 CARDINAL\n",
      "ben PERSON\n",
      "Portland GPE\n",
      "November DATE\n",
      "1959 DATE\n",
      "pps ORG\n",
      "Portland p-tl University n-tl ORG\n",
      "Northwestern jj-tl College n-tl ORG\n",
      "pps ORG\n",
      "three CARDINAL\n",
      "E. p M. p Martin p ORG\n",
      "World n-tl Affairs ORG\n",
      "Council n-tl ORG\n",
      "Monday DATE\n",
      "Martin p ORG\n",
      "Washington GPE\n",
      "D. p C. p ORG\n",
      "13 cd months DATE\n",
      "Multnomah p-tl ORG\n",
      "pps ORG\n",
      "dti ORG\n",
      "350 CARDINAL\n",
      "United vbn-tl FAC\n",
      "States GPE\n",
      "bez congenial ORG\n",
      "pps ORG\n",
      "pps ORG\n",
      "pps ORG\n",
      "jjt PERSON\n",
      "Soviet n-tl NORP\n",
      "Union n-tl ORG\n",
      "pps ORG\n",
      "East LOC\n",
      "Germany GPE\n",
      "15 cd years DATE\n",
      "Soviet NORP\n",
      "pps ORG\n",
      "Martin p ORG\n",
      "Americans NORP\n",
      "a at decade DATE\n",
      "society n we ppss ORG\n",
      "pps ORG\n",
      "United vbn-tl FAC\n",
      "pps ORG\n",
      "best jjt-hl PERSON\n",
      "Martin p ORG\n",
      "United vbn-tl FAC\n",
      "bez more ql ORG\n",
      "pps ORG\n",
      "United vbn-tl FAC\n",
      "pps ORG\n",
      "Martin p ORG\n",
      "one CARDINAL\n",
      "Portland p school n board n ORG\n",
      "Monday DATE\n",
      "Portland GPE\n",
      "board n ORG\n",
      "pps ORG\n",
      "Ralph p H. p Molvar p ORG\n",
      "1409 DATE\n",
      "SW n Maplecrest p Dr. n-tl ORG\n",
      "ppss ORG\n",
      "ppss ORG\n",
      "n-tl PERSON\n",
      "C. p Richard p Mears ORG\n",
      "school n board n ORG\n",
      "n-tl PERSON\n",
      "Melvin p W. p Barnes p ORG\n",
      "pps ORG\n",
      "dti ORG\n",
      "ppss PERSON\n",
      "pps ORG\n",
      "n . GPE\n",
      "ppss ORG\n",
      "pps ORG\n",
      "dti ORG\n",
      "dt PERSON\n",
      "Molvar p ORG\n",
      "pps ORG\n",
      "n-tl PERSON\n",
      "Barnes p ORG\n",
      "Molvar p ORG\n",
      "board n ORG\n",
      "Jack p Lowe's ORG\n",
      "pps ORG\n",
      "pps ORG\n",
      "pps ORG\n",
      "board n ORG\n",
      "bez going vbg ORG\n",
      "pps ORG\n",
      "Salem GPE\n",
      "AP p-hl ORG\n",
      "Tuesday DATE\n",
      "Salem GPE\n",
      "Mark p Hatfield p . ORG\n",
      "Hatfield p ORG\n",
      "Tuesday DATE\n",
      "noon TIME\n",
      "Monday DATE\n",
      "Eugene PERSON\n",
      "Emerald n-tl Empire n-tl ORG\n",
      "Kiwanis p-tl Club n-tl ORG\n",
      "pps ORG\n",
      "Willamette p-tl University n-tl FAC\n",
      "Republicans NORP\n",
      "Thursday DATE\n",
      "Salem GPE\n",
      "p . GPE\n",
      "Friday DATE\n",
      "pps ORG\n",
      "Portland GPE\n",
      "Dean p Bryson p ORG\n",
      "Multnomah p-tl County n-tl Circuit n-tl ORG\n",
      "pps ORG\n",
      "Republican p State n-tl ORG\n",
      "Central jj-tl LOC\n",
      "Committee n-tl ORG\n",
      "Saturday DATE\n",
      "Portland GPE\n",
      "Washington-Oregon p ORG\n",
      "Beaverton p-tl School n-tl District n-tl No ORG\n",
      "48 CARDINAL\n",
      "two CARDINAL\n",
      "Monday DATE\n",
      "two CARDINAL\n",
      "January DATE\n",
      "board n ORG\n",
      "4 CARDINAL\n",
      "3 CARDINAL\n",
      "6-3-3 CARDINAL\n",
      "8-4 CARDINAL\n",
      "Board n ORG\n",
      "Monday DATE\n",
      "Nov. DATE\n",
      "15 CARDINAL\n",
      "581,000 MONEY\n",
      "three CARDINAL\n",
      "n-tl PERSON\n",
      "Labor n-tl ORG\n",
      "Arthur p Goldberg p ORG\n",
      "Sunday DATE\n",
      "Temple n-tl ORG\n",
      "25 MONEY\n",
      "n-tl PERSON\n",
      "Wayne p L. p Morse p ORG\n",
      "Aj n PERSON\n",
      "7 cd p.m. TIME\n",
      "n-tl PERSON\n",
      "Goldberg p ORG\n",
      "n-tl PERSON\n",
      "Morse p ORG\n",
      "Roosevelt p Hotel n-tl FAC\n",
      "4:30 cd p.m. TIME\n",
      "Sunday DATE\n",
      "Blaine p Whipple p ORG\n",
      "Democratic NORP\n",
      "Oregon GPE\n",
      "Tuesday DATE\n",
      "Edith p Green p ORG\n",
      "Al p Ullman p ORG\n",
      "Labor n-tl ORG\n",
      "Norman p Nilsen p ORG\n",
      "Mayor n-tl PERSON\n",
      "Terry p Schrunk p ORG\n",
      "Democrats NORP\n",
      "Oak n-tl-hl PERSON\n",
      "Grove n-tl-hl PERSON\n",
      "Three CARDINAL\n",
      "Oak n-tl FAC\n",
      "Lodge n-tl Water n-tl district n board n ORG\n",
      "11 CARDINAL\n",
      "Dec. p DATE\n",
      "4 CARDINAL\n",
      "8 cd a.m. TIME\n",
      "8 cd p.m. TIME\n",
      "Richard p Salter p ORG\n",
      "Donald p Huffman p ORG\n",
      "five-year DATE\n",
      "William p Brod p PERSON\n",
      "Barbara p Njust p ORG\n",
      "Miles p C. p Bubenik p ORG\n",
      "Frank p Lee p . PERSON\n",
      "Five CARDINAL\n",
      "Hugh p G. p Stout p . ORG\n",
      "two-year DATE\n",
      "James p Culbertson p ORG\n",
      "Dwight p M. p Steeves p ORG\n",
      "James p C. p Piersee p ORG\n",
      "W.M. p Sexton p ORG\n",
      "Theodore p ORG\n",
      "p . GPE\n",
      "Friday DATE\n",
      "29th DATE\n",
      "general jj council n ORG\n",
      "Assemblies ns-tl FAC\n",
      "Memorial jj-tl FAC\n",
      "Coliseum p-tl ORG\n",
      "The at council n ORG\n",
      "16 CARDINAL\n",
      "1966 DATE\n",
      "Bible p . FAC\n",
      "council n ORG\n",
      "pps ORG\n",
      "Bible p . FAC\n",
      "The at-tl Assemblies ns-tl of in-tl God p WORK_OF_ART\n",
      "days DATE\n",
      "Bible p WORK_OF_ART\n",
      "first ORDINAL\n",
      "third ORDINAL\n",
      "first ORDINAL\n",
      "16 CARDINAL\n",
      "third ORDINAL\n",
      "six CARDINAL\n",
      "Diety n-tl ORG\n",
      "Jesus p Christ p ORG\n",
      "Virgin n-tl FAC\n",
      "Christ p \n",
      "\n",
      "\t ORG\n",
      "cross n \n",
      "\n",
      "\t ORG\n",
      "Super n-hl ORG\n",
      "Friday DATE\n",
      "afternoon TIME\n",
      "the at Rev. p T. p F. p Zimmerman p ORG\n",
      "second ORDINAL\n",
      "two-year DATE\n",
      "Assemblies GPE\n",
      "pp$ PERSON\n",
      "Springfield GPE\n",
      "Mo. p . ORG\n",
      "Friday DATE\n",
      "Breakthrough n-tl WORK_OF_ART\n",
      "two cd years DATE\n",
      "Wednesday DATE\n",
      "Rev. p ORG\n",
      "Zimmerman p ORG\n",
      "10-year DATE\n",
      "Breakthrough n-tl WORK_OF_ART\n",
      "first ORDINAL\n",
      "two cd years DATE\n",
      "first ORDINAL\n",
      "two cd years' ns$ DATE\n",
      "one CARDINAL\n",
      "three CARDINAL\n",
      "Americans NORP\n",
      "church n ORG\n",
      "100 cd million CARDINAL\n",
      "pps ORG\n",
      "Friday DATE\n",
      "Church n-hl ORG\n",
      "12,000 CARDINAL\n",
      "daily DATE\n",
      "United vbn-tl FAC\n",
      "1-1 2 CARDINAL\n",
      "pps ORG\n",
      "35 cd years DATE\n",
      "7,000 CARDINAL\n",
      "Rev. p ORG\n",
      "Brandt p ORG\n",
      "one CARDINAL\n",
      "10,000 CARDINAL\n",
      "dt PRODUCT\n",
      "1,000 CARDINAL\n",
      "Illinois p ORG\n",
      "200 CARDINAL\n",
      "800 CARDINAL\n",
      "Southern jj-tl LOC\n",
      "England p ORG\n",
      "60 CARDINAL\n",
      "100 CARDINAL\n",
      "Rhode p-tl Island n-tl GPE\n",
      "pps ORG\n",
      "Rev. p ORG\n",
      "Brandt p ORG\n",
      "8,000 CARDINAL\n",
      "Assemblies PERSON\n",
      "10 cd years DATE\n",
      "dti ORG\n",
      "pps ORG\n",
      "this dt hour TIME\n",
      "6-12 CARDINAL\n",
      "U.S. GPE\n",
      "n-tl PERSON\n",
      "Charles p L. p Powell ORG\n",
      "Monday DATE\n",
      "Portland GPE\n",
      "n-tl PERSON\n",
      "Powell p ORG\n",
      "md join GPE\n",
      "Dwight p L. p Schwab p ORG\n",
      "Philip p Weinstein p ORG\n",
      "Weinstein PERSON\n",
      "n-tl PERSON\n",
      "Powell p ORG\n",
      "Proof n-hl PERSON\n",
      "Schwab PERSON\n",
      "Weinstein PERSON\n",
      "U.S. GPE\n",
      "U.S. GPE\n",
      "afternoon TIME\n"
     ]
    }
   ],
   "source": [
    "text0=NER(newsentence)\n",
    "for word in text0.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ba040d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\"\n",
    "text1=NER(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c5a04ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Indian Space Research Organisation ORG\n",
      "India GPE\n",
      "Bengaluru GPE\n",
      "Department of Space ORG\n",
      "India GPE\n",
      "ISRO ORG\n",
      "DOS ORG\n"
     ]
    }
   ],
   "source": [
    "for word in text1.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab40e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  (NP 61/CD years/NNS)\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  (NP the/DT board/NN)\n",
      "  as/IN\n",
      "  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n",
      "  ./.)\n",
      "[('the', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
      "(S\n",
      "  (NP the/DT brown/JJ fox/NN)\n",
      "  (VP jumped/VBD over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "(S\n",
      "  the/DT\n",
      "  (NP brown/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  the/DT\n",
      "  (NP lazy/JJ dog/NN))\n",
      "(S\n",
      "  (NP the/DT brown/JJ fox/NN)\n",
      "  (VP is/VBZ)\n",
      "  (ADJP quick/JJ)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  (VP may/MD jump/VB)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16184\\2397733014.py:68: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(rc.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  46.1%%\n",
      "    Precision:     19.9%%\n",
      "    Recall:        43.3%%\n",
      "    F-Measure:     27.3%%\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16184\\2397733014.py:124: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(ntc.evaluate(test_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  97.2%%\n",
      "    Precision:     91.4%%\n",
      "    Recall:        94.3%%\n",
      "    F-Measure:     92.8%%\n",
      "(S\n",
      "  (NP the/DT brown/JJ fox/NN)\n",
      "  is/VBZ\n",
      "  (NP quick/JJ)\n",
      "  and/CC\n",
      "  (NP he/PRP)\n",
      "  may/MD\n",
      "  jump/VB\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16184\\2397733014.py:146: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  print(tc.evaluate(test_wsj_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.4%%\n",
      "    Precision:     80.8%%\n",
      "    Recall:        86.0%%\n",
      "    F-Measure:     83.3%%\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created July 2017\n",
    "\n",
    "@author: arw\n",
    "\"\"\"\n",
    "\n",
    "# Training your own chunker using chunked treeband data - again made available in NLTK!\n",
    "# As before (with tagging) we first divide the data into training and testing sets\n",
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[0:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data[0])\n",
    "\n",
    "simple_sentence = 'the brown fox jumped over the lazy dog'\n",
    "\n",
    "# Can use tagger from package pattern.en if using Python 2.x\n",
    "# from pattern.en import tag\n",
    "# tagged_sentence = tag(sentence)\n",
    "\n",
    "import nltk\n",
    "from nltk.chunk import RegexpParser\n",
    "tokens = nltk.word_tokenize(simple_sentence)\n",
    "tagged_simple_sent = nltk.pos_tag(tokens)\n",
    "print(tagged_simple_sent)\n",
    "\n",
    "# We first define our grammars using regex pattern using the RegexpParser\n",
    "# We can specify which patterns we want to segment in a sentence as *chunks*\n",
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "VP: {<VBD><IN>}\n",
    "\"\"\"\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)\n",
    "\n",
    "# We sometimes want to specify which patterns we DO NOT want to segment in a sentence\n",
    "# so that we can *chunk* all the others\n",
    "chink_grammar = \"\"\"\n",
    "NP: {<JJ|NN>+} # chunk only adjective-noun pair as NP\n",
    "\"\"\"\n",
    "\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)\n",
    "\n",
    "\n",
    "# A more realistic grammar for chunking\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}  \n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}      \n",
    "VP: {<MD>?<VB.*>+}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# And a more realistic sentence as input\n",
    "sentence = 'the brown fox is quick and he may jump over the lazy dog'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens)\n",
    "\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_sent)\n",
    "print(c)\n",
    "\n",
    "print(rc.evaluate(test_data))\n",
    "# The performance is not great!\n",
    "# Why is this?\n",
    "\n",
    "\n",
    "# We have acecss to a utility function tree2conlltags which extracts word, tag and\n",
    "# chunk triples from annotated text\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "# Let's take a slightly more typical sentence from our data\n",
    "train_sent = train_data[7]\n",
    "print(train_sent)\n",
    "\n",
    "# We extract the POS and chunk tags using tree2conlltags function which returns a list of tuples\n",
    "wtc = tree2conlltags(train_sent)\n",
    "wtc\n",
    "\n",
    "# We can 'reverse' this to output a shallow tree using the conlltags2tree function\n",
    "tree = conlltags2tree(wtc)\n",
    "print(tree)\n",
    "\n",
    "\n",
    "# We can use these features to train a 'combined' chunker as we did for POS tagging\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "  tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "  \n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "  \n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "# We create a new class to use the word, POS and Chunk tag features to train a chunker\n",
    "# that is able to 'backoff' from bigram to a unigram model as before\n",
    "# Can you have another layer for trigram and back off to this model?\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    \n",
    "  def __init__(self, train_sentences, \n",
    "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "  def parse(self, tagged_sentence):\n",
    "    if not tagged_sentence: \n",
    "        return None\n",
    "    pos_tags = [tag for word, tag in tagged_sentence]\n",
    "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                     in zip(tagged_sentence, chunk_tags)]\n",
    "    return conlltags2tree(wpc_tags)\n",
    "\n",
    "# We call our new class and pass it the training data from the chunked treebank\n",
    "ntc = NGramTagChunker(train_data)\n",
    "print(ntc.evaluate(test_data))\n",
    "# Now we get really good results on the data set\n",
    "# Why?\n",
    "\n",
    "# Let's try to visualize the chunk for the more realistic sample sentence\n",
    "tree = ntc.parse(tagged_sent)\n",
    "print(tree)\n",
    "tree.draw()\n",
    "\n",
    "\n",
    "# We now use our shallow parser on a larger 'Wall Street Journal' corpus\n",
    "# SAQ 1. How big is it?\n",
    "# SAQ 2. How much test data did we have before, and how much now?\n",
    "from nltk.corpus import conll2000\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:7500]\n",
    "test_wsj_data = wsj_data[7500:]\n",
    "print(train_wsj_data[10])\n",
    "\n",
    "# We first train our model on the training corpus\n",
    "tc = NGramTagChunker(train_wsj_data)\n",
    "# And then we test it on the test data\n",
    "print(tc.evaluate(test_wsj_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0422ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
